{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD($\\lambda$) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 and 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Alex/Desktop/Documents_4A/Winter_quarter_1/MS&E_346/RL_book/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.511,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345}\n"
     ]
    }
   ],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite\n",
    "user_capacity = 2\n",
    "user_poisson_lambda = 1.0\n",
    "user_holding_cost = 1.0\n",
    "user_stockout_cost = 10.0\n",
    "user_gamma = 0.9\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=user_capacity,\n",
    "    poisson_lambda=user_poisson_lambda,\n",
    "    holding_cost=user_holding_cost,\n",
    "    stockout_cost=user_stockout_cost\n",
    ")\n",
    "si_mrp.display_value_function(gamma=user_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.821,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.333,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.62,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.166,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.223,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.249}\n"
     ]
    }
   ],
   "source": [
    "## MY test rep\n",
    "import rl.iterate as iterate\n",
    "import rl.td_lambda as td_lambda\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from rl.chapter10.prediction_utils import fmrp_episodes_stream\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.markov_process import TransitionStep\n",
    "\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "    \n",
    "it: Iterator[ValueFunctionApprox[InventoryState]] = td_lambda_prediction(\n",
    "    traces=traces,\n",
    "    approx_0=Tabular(count_to_weight_func=learning_rate_func),\n",
    "    gamma=user_gamma,\n",
    "    lambd=0.3\n",
    ")\n",
    "    \n",
    "print(\"test\")\n",
    "num_transitions = 20000\n",
    "last_func: ValueFunctionApprox[InventoryState] = last(islice(it, num_transitions))\n",
    "    \n",
    "pprint({s: round(last_func.evaluate([s])[0], 3) for s in si_mrp.non_terminal_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.873,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.504,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.818,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.368,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.425,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.555}\n"
     ]
    }
   ],
   "source": [
    "# Implementation with repository\n",
    "import rl.iterate as iterate\n",
    "import rl.td_lambda as td_lambda\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from rl.chapter10.prediction_utils import fmrp_episodes_stream\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.markov_process import TransitionStep\n",
    "\n",
    "\n",
    "gamma: float = 0.9\n",
    "episode_length: int = 100\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5\n",
    "lambda_param = 0.3\n",
    "\n",
    "episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "    fmrp_episodes_stream(si_mrp)\n",
    "    \n",
    "curtailed_episodes: Iterable[Iterable[TransitionStep[S]]] = \\\n",
    "        (itertools.islice(episode, episode_length) for episode in episodes)\n",
    "\n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "\n",
    "td_lambda_vfs: Iterator[ValueFunctionApprox[S]] = td_lambda.td_lambda_prediction(\n",
    "    traces=curtailed_episodes,\n",
    "    approx_0=Tabular(count_to_weight_func=learning_rate_func),\n",
    "    gamma=gamma,\n",
    "    lambd=lambda_param\n",
    ")\n",
    "    \n",
    "num_episodes = 1000\n",
    "\n",
    "final_td_lambda_vf: ValueFunctionApprox[S] = \\\n",
    "    iterate.last(itertools.islice(td_lambda_vfs, episode_length * num_episodes))\n",
    "    \n",
    "pprint({s: round(final_td_lambda_vf(s), 3) for s in si_mrp.non_terminal_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter2.simple_inventory_mrp import InventoryState\n",
    "from rl.function_approx import Tabular\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.distribution import Choose\n",
    "from rl.iterate import last\n",
    "from rl.monte_carlo import mc_prediction\n",
    "from itertools import islice\n",
    "from pprint import pprint\n",
    "from typing import Iterable, Iterator, TypeVar, Callable\n",
    "\n",
    "traces = si_mrp.reward_traces(Choose(si_mrp.non_terminal_states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Tuple, Mapping, TypeVar, Iterator, Sequence\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "from itertools import *\n",
    "from numpy.random import randint\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "S = TypeVar('S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rl.iterate as iterate\n",
    "import rl.td as td\n",
    "import itertools\n",
    "from pprint import pprint\n",
    "from rl.chapter10.prediction_utils import fmrp_episodes_stream\n",
    "from rl.chapter10.prediction_utils import unit_experiences_from_episodes\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.markov_process import TransitionStep\n",
    "\n",
    "user_gamma: float = 0.9\n",
    "episode_length: int = 100\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tabular from scratch : no terminal state\n",
    "episodes: Iterable[Iterable[TransitionStep[S]]] = fmrp_episodes_stream(si_mrp)\n",
    "    \n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "\n",
    "def get_td_lamb_value_function(episodes, num_episodes, episode_length, lr, gamma, mp,lamb):\n",
    "    vc : Mapping[S,float] = {s : 0. for s in mp.non_terminal_states}\n",
    "    count_lr=0\n",
    "    count_episode = 0\n",
    "    for episode in episodes : \n",
    "        if count_episode > num_episodes : \n",
    "            break\n",
    "        eligibility_trace = {s : 0. for s in mp.non_terminal_states}\n",
    "        list_exp = list(itertools.islice(episode,episode_length))\n",
    "        T0=list_exp[0]\n",
    "        eligibility_trace[T0.state] = 1\n",
    "        for T in list_exp :\n",
    "            for s in mp.non_terminal_states:\n",
    "                vc[s] = vc[s] + lr(count_lr)*(T.reward + gamma*vc[T.next_state] - vc[T.state])*eligibility_trace[s]\n",
    "                count_lr += 1\n",
    "                eligibility_trace[s] = gamma*lamb*eligibility_trace[s] + (T.next_state==s)\n",
    "        count_episode +=1\n",
    "    return vc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- Temporal Difference VALUE FUNCTION --------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.53156545109951,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.988479977722708,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.380546342792623,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.006590551672453,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.39613749402256,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.436358920574946}\n"
     ]
    }
   ],
   "source": [
    "td_lamb_val = get_td_lamb_value_function(episodes=episodes, num_episodes = 2000, episode_length=200, lr=learning_rate_func, gamma=user_gamma, mp=si_mrp,lamb=0.3)\n",
    "\n",
    "print(\"------------- Temporal Difference VALUE FUNCTION --------------\")\n",
    "pprint(td_lamb_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works great (better than only MC or only TD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Relation between MC and TD error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a trace $(S_0,...,S_t,...,S_T)$ with finite horizon $T$, and $V$ is the value function. Thus $S_T$ is a terminal state, and so we have that $V(S_T) = 0$\n",
    "\n",
    "For a time $0\\leq t<T$, the MC error is $G_t - V(S_t)$, with $G_t$ the sum of discounted rewards : $G_t = R_{t+1} + \\gamma* G_{t+1}$\n",
    "\n",
    "Let's prove the result by backward induction :\n",
    "\n",
    "#### Initialization\n",
    "At time $t=T-1$, we have that $V(S_{t+1}) = V(S_T) = 0$, and thus $G_{T-1} - V(S_{T-1}) = R_T + \\gamma*V(S_T) - V(S_{T-1})$\n",
    "\n",
    "\n",
    "#### Heredity\n",
    "\n",
    "Let's consider a time t < T-1 and assume the induction hypothesis true for t+1.\n",
    "\n",
    "Then $$G_t - V(S_t) = R_{t+1} + \\gamma * G_{t+1} - V(S_t) = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) + \\gamma * (G_{t+1} - V(S_{t+1}))$$\n",
    "\n",
    "Using the induction hypothesis, $$G_t - V(S_t) = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) + \\sum_{u=t+1}^{T-1}\\gamma^{u-(t+1)+1}*(R_{u+1} + \\gamma V(S_{u+1}) - V(S_u)) = \\sum_{u=t}^{T-1}\\gamma^{u-t}*(R_{u+1} + \\gamma V(S_{u+1}) - V(S_u))$$\n",
    "\n",
    "We have proven initialization and heredity, the equality is proven."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
