{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 : Policy gradient and score function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $log(\\pi(s,a,\\theta) = \\phi(s,a)^T\\theta - log(\\sum_b e^{\\phi(s,b)^T\\theta})$\n",
    "\n",
    "Then $\\nabla_\\theta log(\\pi(s,a,\\theta) = \\phi(s,a) - \\sum_b \\phi(s,b)\\pi(s,b,\\theta) = \\phi(s,a) - E_\\pi[\\phi(s,.)]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enable compatible function approximation we make the Action Q-Value function $Q(s,a,w)$ a linear function approximation of the Score of the policy : \n",
    "\n",
    "$$Q(s,a,w) = w^T \\nabla_\\theta log(\\pi(s,a,\\theta)$$\n",
    "\n",
    "Then $\\nabla_w Q(s,a,w) = \\nabla_\\theta log(\\pi(s,a,\\theta)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $E_\\pi[Q(s,a,w)] = \\sum_a \\pi(s,a,\\theta)Q(s,a,w) = w^T \\sum_a \\pi(s,a,\\theta)\\nabla_\\theta log(\\pi(s,a,\\theta) $\n",
    "\n",
    "But $\\sum_a \\pi(s,a,\\theta)\\nabla_\\theta log(\\pi(s,a,\\theta) = E_\\pi[\\phi(s,.)] - E_\\pi[E_\\pi[\\phi(s,.)]] = 0$\n",
    "\n",
    "Then $Q(s, a; w)$ has zero mean for any state s ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
