{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/Users/Alex/Desktop/Documents_4A/Winter_quarter_1/MS&E_346/RL_book/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Tabular GLIE Monte Carlo Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "gamma: float = 0.9\n",
    "si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from rl.dynamic_programming import value_iteration_result\n",
    "from pprint import pprint\n",
    "\n",
    "true_opt_vf, true_opt_policy = value_iteration_result(si_mdp, gamma=gamma)\n",
    "\n",
    "print(\"True Optimal Value Function\")\n",
    "pprint(true_opt_vf)\n",
    "print(\"True Optimal Policy\")\n",
    "print(true_opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run from repo code\n",
    "\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.chapter3.simple_inventory_mdp_cap import InventoryState\n",
    "from typing import Iterable, Iterator, TypeVar, Callable, Mapping, Tuple\n",
    "from rl.markov_process import TransitionStep, NonTerminal, FiniteMarkovProcess\n",
    "from rl.chapter10.prediction_utils import fmrp_episodes_stream\n",
    "from rl.chapter10.prediction_utils import unit_experiences_from_episodes\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.monte_carlo import glie_mc_control\n",
    "from rl.approximate_dynamic_programming import (ValueFunctionApprox,\n",
    "                                                QValueFunctionApprox,\n",
    "                                                NTStateDistribution)\n",
    "\n",
    "\n",
    "episode_length_tolerance: float = 1e-5\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "    \n",
    "initial_qvf_dict: Mapping[Tuple[NonTerminal[InventoryState], int], float] = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "    \n",
    "learning_rate_func: Callable[[int], float] = learning_rate_schedule(\n",
    "    initial_learning_rate=initial_learning_rate,\n",
    "    half_life=half_life,\n",
    "    exponent=exponent\n",
    ")\n",
    "    \n",
    "qvfs: Iterator[QValueFunctionApprox[InventoryState, int]] = glie_mc_control(\n",
    "    mdp=si_mdp,\n",
    "    states=Choose(si_mdp.non_terminal_states),\n",
    "    approx_0=Tabular(\n",
    "        values_map=initial_qvf_dict,\n",
    "        count_to_weight_func=learning_rate_func\n",
    "    ),\n",
    "    gamma=gamma,\n",
    "    epsilon_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "    episode_length_tolerance=episode_length_tolerance\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with 1000 episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.26305099605248,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.05856049295257,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.666669208210045,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.02018798023223,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.56475740041169,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.8983792198719}\n",
      "GLIE MC Optimal Policy with 1000 episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#fetch the last iteration\n",
    "\n",
    "from rl.distribution import Constant\n",
    "from rl.dynamic_programming import V\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "num_episodes = 1000\n",
    "\n",
    "final_qvf: QValueFunctionApprox[InventoryState, int] = iterate.last(itertools.islice(qvfs, num_episodes))\n",
    "    \n",
    "def get_vf_and_policy_from_qvf(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    qvf: QValueFunctionApprox[S, A]\n",
    ") -> Tuple[V[S], FiniteDeterministicPolicy[S, A]]:\n",
    "    opt_vf: V[S] = {\n",
    "        s: max(qvf((s, a)) for a in mdp.actions(s))\n",
    "        for s in mdp.non_terminal_states\n",
    "    }\n",
    "    opt_policy: FiniteDeterministicPolicy[S, A] = \\\n",
    "        FiniteDeterministicPolicy({\n",
    "            s.state: qvf.argmax((s, a) for a in mdp.actions(s))[1]\n",
    "            for s in mdp.non_terminal_states\n",
    "        })\n",
    "    return opt_vf, opt_policy\n",
    "\n",
    "opt_vf, opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=final_qvf\n",
    ")\n",
    "\n",
    "print(f\"GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(opt_vf)\n",
    "print(f\"GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tabular from scratch\n",
    "\n",
    "from typing import Sequence\n",
    "from rl.returns import returns\n",
    "from rl.monte_carlo import epsilon_greedy_policy\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "episode_length_tolerance: float = 1e-5\n",
    "epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "initial_learning_rate: float = 0.1\n",
    "half_life: float = 10000.0\n",
    "exponent: float = 1.0\n",
    "\n",
    "\n",
    "def get_returns(trace,gamma,episode_length_tolerance):\n",
    "    l = []\n",
    "    trace = iter(trace)\n",
    "    max_steps = int(np.log(episode_length_tolerance) / np.log(gamma)) if gamma < 1 else None\n",
    "    if max_steps is not None:\n",
    "        trace = list(itertools.islice(trace, int(max_steps * 2) ))\n",
    "    for i, T in enumerate(trace): #T is transition step type \n",
    "        if i < max_steps :\n",
    "            ret = sum(A.reward*(gamma**q) for q,A in enumerate(trace[i:]))\n",
    "            l.append((T.state,ret))\n",
    "    return l\n",
    "\n",
    "def get_glie_mc_control(mdp,initial_qf,max_episodes,gamma, episode_length_tolerance):\n",
    "    qf : QValueFunctionApprox[S,A] = Tabular(values_map=initial_qvf_dict)\n",
    "    p: Policy[S, A] = epsilon_greedy_policy(qf, mdp, 1.0)\n",
    "    nb_episodes = 0\n",
    "    count : Mapping[Tuple[State,Action],float] = {(s, a): 0. for s in mdp.non_terminal_states for a in si_mdp.actions(s)}\n",
    "    while True:\n",
    "        trace: Iterable[TransitionStep[S, A]] = mdp.simulate_actions(Choose(mdp.non_terminal_states), p)\n",
    "        nb_episodes +=1\n",
    "        if nb_episodes > max_episodes :\n",
    "            break\n",
    "        qf_dict = dict(qf.values_map)\n",
    "        for step in returns(trace, gamma, episode_length_tolerance) :\n",
    "            count[(step.state,step.action)] += 1\n",
    "            qf_dict = dict(qf.values_map)\n",
    "            qf_dict[(step.state,step.action)] = qf_dict[(step.state,step.action)] + (step.return_ - qf_dict[(step.state,step.action)])/count[(step.state,step.action)]\n",
    "        qf = Tabular(values_map = qf_dict)\n",
    "        p = epsilon_greedy_policy(qf, mdp, epsilon_as_func_of_episodes(num_episodes))\n",
    "    return qf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.function_approx import FunctionApprox\n",
    "\n",
    "QValueFunctionApprox = FunctionApprox[Tuple[NonTerminal[S], A]]\n",
    "\n",
    "initial_qf_dict: Mapping[Tuple[NonTerminal[InventoryState], int], float] = {\n",
    "    (s, a): 0. for s in si_mdp.non_terminal_states for a in si_mdp.actions(s)\n",
    "}\n",
    "    \n",
    "glie_mc = get_glie_mc_control(si_mdp,initial_qf = initial_qf_dict,max_episodes =1000,gamma = gamma,episode_length_tolerance=episode_length_tolerance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MY GLIE MC Optimal Value Function with 1000 episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -1.9385957175473876,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -2.2435490662160076,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -2.8510210598685144,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -0.6403732022785171,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -1.034839098782903,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -1.898992933665553}\n",
      "MY GLIE MC Optimal Policy with 1000 episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_opt_vf, my_opt_policy = get_vf_and_policy_from_qvf(\n",
    "    mdp=si_mdp,\n",
    "    qvf=glie_mc\n",
    ")\n",
    "\n",
    "print(f\"MY GLIE MC Optimal Value Function with {num_episodes:d} episodes\")\n",
    "pprint(my_opt_vf)\n",
    "print(f\"MY GLIE MC Optimal Policy with {num_episodes:d} episodes\")\n",
    "print(my_opt_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
